# -*- coding: utf-8 -*-
"""KNN on Amazon Food Reviews Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kp-cOUMnW861qpmWenE8apdPUKq01ap3

### Aim : Predict/classify the new text/review given by the user as Positive or Neutral or Critical using KNN Classifier

Following Steps will be followed to acheive the aim.
1. Loading of Amazon review Dataset and Data Cleaning and Pre processing
2. Using TF-IDF weighted word2Vec algorithm to convert Texts in vector form
3. KNN Model Building:
  > Spliting the dataset into Train,Cross validation and Test Dataset

  > Training KNN Model using Train dataset.

  > Tuning Hyper parameters of KNN using Cross validation dataset.

  > Retraining the model with the optimal value of hyper parameters.

  > Calculating the Accuracy of model using Test dataset.
"""

# ============================== loading libraries ===========================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from collections import Counter
from sklearn.metrics import accuracy_score
from sklearn import model_selection
# =============================================================================================

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks/KNN Classifier on Amazon Food Reviews')

# !cd "/content/drive/MyDrive/Colab Notebooks/KNN Classifier on Amazon Food Reviews"

from featurization import *

# Part I

# create design matrix X and target vector y
#  All input features
# X = np.array(vectorized_text_df.iloc[:, :-1]) # select all colums except the last column 

# X = np.array(vectorized_text_df.iloc[:,1]) # select column at index 1 because thats our vector form of Text
# # Target column
# y = np.array(vectorized_text_df['Score']) # showing you two ways of indexing a pandas df

X = np.array(vectorized_text_df['tfidf_sent_vectors'].to_list())
y = np.array(vectorized_text_df['Score'].to_list())

"""### Tuning Hyperparamater K: Simple Cross Validation ###
####Tuning Hyperparamater K to find optimal value of K to avoid Overfitting / Underfitting of model####
"""

# split the data set into train and test
X_1, X_test, y_1, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)

# split the train data set into cross validation train and cross validation test
X_tr, X_cv, y_tr, y_cv = model_selection.train_test_split(X_1, y_1, test_size=0.3)

for i in range(1,30,2):
    # instantiate learning model (k = 30)
    knn = KNeighborsClassifier(n_neighbors=i)

    # fitting the model on train dataset
    knn.fit(X_tr, y_tr)

    # predict the response(predicd = predicted y_cv) on the 
    # crossvalidation dataset(X_cv) on the above trained model on training data with K = i
    pred = knn.predict(X_cv)

    # evaluate CV accuracy on K = i
    acc = accuracy_score(y_cv, pred, normalize=True) * float(100)
    print('\nCV accuracy for k = %d is %d%%' % (i, acc))

"""#### Retraining with the optimal value of K received using simple cross validation and calculating test accuracy ####"""

print(type(X_test))

# Retrain the model with tuned hyperparmeter Koptimal (Koptimal = K where accuracy is max in CV) 
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_tr,y_tr)
pred = knn.predict(X_test)
acc = accuracy_score(y_test, pred, normalize=True) * float(100)
print('\n****Test accuracy for k = 1 is %d%%' % (acc))

#  Predicted class from KNN

# Get text corresponding to vectors
# actual_text = vectorized_text_df.loc[vectorized_text_df['tfidf_sent_vectors'].isin(list(X_test))]['Text']
# print(actual_text)

model_predicted_on_test_data = pd.DataFrame()
model_predicted_on_test_data['Text'] = list(X_test)
model_predicted_on_test_data['Actual Class'] = y_test
model_predicted_on_test_data['Predicted Class'] = pred

model_predicted_on_test_data.to_csv(r"KNN_Predicted_Class.csv",index=False)
model_predicted_on_test_data.head(5)

"""### 10 fold Cross vaildation ###"""

# creating odd list of K for KNN
myList = list(range(0,50))
neighbors = list(filter(lambda x: x % 2 != 0, myList))

# empty list that will hold cv scores
cv_scores = []

# perform CV = 10-fold cross validation
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_tr, y_tr, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# changing to misclassification error
MSE = [1 - x for x in cv_scores]

# determining best k
optimal_k = neighbors[MSE.index(min(MSE))]
print('\nThe optimal number of neighbors is %d.' % optimal_k)

# plot misclassification error vs k 
plt.plot(neighbors, MSE)

for xy in zip(neighbors, np.round(MSE,3)):
    plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')

plt.xlabel('Number of Neighbors K')
plt.ylabel('Misclassification Error')
plt.show()

print("the misclassification error for each k value is : ", np.round(MSE,3))

"""#### Retraining with the optimal value of K received using 10 fold cross validation and calculating test accuracy####"""

# ============================== KNN with k = optimal_k ===============================================
# instantiate learning model k = optimal_k
knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)

# fitting the model
knn_optimal.fit(X_tr, y_tr)

# predict the response
pred = knn_optimal.predict(X_test)

# evaluate accuracy
acc = accuracy_score(y_test, pred) * 100
print('\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))

#  Predicted class from KNN
# model_predicted_on_test_data = pd.DataFrame()
# model_predicted_on_test_data['Text'] = list(X_test)
# model_predicted_on_test_data['Actual Class'] = y_test
# model_predicted_on_test_data['Predicted Class'] = pred

# model_predicted_on_test_data.to_csv(r"KNN_Predicted_Class.csv",index=False)
# model_predicted_on_test_data.head(5)

# from sklearn.model_selection import cross_val_score
# from sklearn import datasets, linear_model
# from sklearn.neighbors import KNeighborsClassifier
# diabetes = datasets.load_diabetes()
# X = diabetes.data[:150]
# y = diabetes.target[:150]
# # creating odd list of K for KNN
# myList = list(range(0,10))
# neighbors = list(filter(lambda x: x % 2 != 0, myList))

# # empty list that will hold cv scores
# cv_scores = []

# # perform 10-fold cross validation
# for k in neighbors:
#     print(k)
#     knn = KNeighborsClassifier(n_neighbors=k)
    
#     scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
#     print(X)
#     cv_scores.append(scores.mean())

