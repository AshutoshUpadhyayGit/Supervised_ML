# -*- coding: utf-8 -*-
"""Featurization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uUjUJ5uDk6cjZ-vJ_OK3D3AXNrNM6-b9

# Converting the Processed Texts of Amazon food Reviews to a Vector using "TF IDF weighted Word2Vec" algorithm.
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks/KNN Classifier on Amazon Food Reviews')

# !cd "/content/drive/MyDrive/Colab Notebooks/KNN Classifier on Amazon Food Reviews"

from loading_of_amazon_review_dataset_and_data_cleaning_and_pre_processing import *

# S = ["abc def pqr", "def def def abc", "pqr pqr def"]
model = TfidfVectorizer()
model.fit(preprocessed_reviews)
# we are converting a dictionary with word as a key, and the idf as a value
dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))

# Train your own Word2Vec model using your own text corpus
i=0
list_of_sentance=[]
for sentance in preprocessed_reviews:
    list_of_sentance.append(sentance.split())

# !{sys.executable} -m pip install --upgrade gensim
# Output: Successfully installed gensim-4.1.2

# Using Google News Word2Vectors

# in this project we are using a pretrained model by google
# its 3.3G file, once you load this into your memory 
# it occupies ~9Gb, so please do this step only if you have >12G of ram
# we will provide a pickle file wich contains a dict , 
# and it contains all our courpus words as keys and  model[word] as values
# To use this code-snippet, download "GoogleNews-vectors-negative300.bin" 
# from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
# it's 1.9GB in size.


# http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W17SRFAzZPY
# you can comment this whole cell
# or change these varible according to your need

################################### For use of bigrams in word2vec : Phrases ##########################
from gensim.models import Phrases

bigram_transformer = Phrases(list_of_sentance)
# then use "bigram_transformer[list_of_sentance]" as input to Word2Vec model
########################################################################################################

is_your_ram_gt_16g=False
want_to_use_google_w2v = False
want_to_train_w2v = True

if want_to_train_w2v:
    # min_count = 5 considers only words that occured atleast 5 times
    # size = dimesnion of vector     
    # workers = No of logical cores of PC for training model
    
    # Model is being trained and for each word in sentence, a 50 dimensional vector is created
    w2v_model=Word2Vec(bigram_transformer[list_of_sentance],min_count=5,size=50, workers=4)
    # w2v_model=Word2Vec(bigram_transformer[list_of_sentance],min_count=5,vector_size=50, workers=4) ---- Latest changes vector_size
    print(w2v_model.wv.most_similar('great'))
    print('='*50)
    print(w2v_model.wv.most_similar('worst'))
    
# Output of above if block:
# [('excellent', 0.9872738718986511), ('alternative', 0.9829381704330444), ('snack', 0.9825526475906372), ('calorie', 0.9814417362213135), ('wonderful', 0.9812389016151428), ('especially', 0.9809868335723877), ('healthier', 0.9806246161460876), ('regular', 0.9799271821975708), ('snacking', 0.979718029499054), ('seriously', 0.979637622833252)]
# ==================================================
# [('clear', 0.9981780052185059), ('stand', 0.9981135725975037), ('american', 0.9980722665786743), ('double', 0.997939944267273), ('terrible', 0.9979246854782104), ('remember', 0.9979243874549866), ('stash', 0.9978914260864258), ('peanuts', 0.9978435635566711), ('fairly', 0.9978325963020325), ('turned', 0.9978171586990356)]
    
elif want_to_use_google_w2v and is_your_ram_gt_16g:
    if os.path.isfile('GoogleNews-vectors-negative300.bin'):
        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
        print(w2v_model.wv.most_similar('great'))
        print(w2v_model.wv.most_similar('worst'))
    else:
        print("you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v ")

# w2v_words = list(w2v_model.wv.key_to_index)

w2v_words = list(w2v_model.wv.vocab)
print(w2v_words)

################ Saving the mappings of words to its corresponding vectors in file #######################
w2v_words_Save = w2v_model.wv
w2v_words_Save.save("mapping_word2vec")

# TF-IDF weighted Word2Vec
tfidf_feat = model.get_feature_names() # tfidf words/col-names
# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf

tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list
row=0;
for sent in tqdm(list_of_sentance): # for each review/sentence 
    sent_vec = np.zeros(50) # as word vectors are of zero length
    weight_sum =0; # num of words with a valid vector in the sentence/review
    for word in sent: # for each word in a review/sentence
        if word in w2v_words and word in tfidf_feat:
            vec = w2v_model.wv[word]
#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]
            # to reduce the computation we are 
            # dictionary[word] = idf value of word in whole courpus
            # sent.count(word) = tf valeus of word in this review
            tf_idf = dictionary[word]*(sent.count(word)/len(sent))
            sent_vec += (vec * tf_idf)
            weight_sum += tf_idf
    if weight_sum != 0:
        sent_vec /= weight_sum
    tfidf_sent_vectors.append(sent_vec)
    row += 1
    
############################ mapping of sentence and its corresponding vector ###########################
print("*" * 50)
import pandas as pd
vectorized_text_df = pd.DataFrame()
vectorized_text_df['Text'] = preprocessed_reviews_df['Text']
vectorized_text_df['preprocessed_reviews'] = preprocessed_reviews
vectorized_text_df['tfidf_sent_vectors'] = tfidf_sent_vectors
vectorized_text_df['Score'] = preprocessed_reviews_df['Score']
print(vectorized_text_df.head(5))
print("*" * 50)

# df.to_csv("/content/drive/MyDrive/Colab Notebooks/KNN Classifier on Amazon Food Reviews/text_to_Vector.csv",index=False)